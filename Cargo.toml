[package]
name = "website_crawler"
version = "0.2.1"
authors = ["jeffreymendez <jeffmendez19@gmail.com>"]
edition = "2021"
description = "crawl all urls on a website async & sync"
license = "MIT"
readme = "README.md"
repository = "https://github.com/A11yWatch/crawler"
keywords = ["crawler", "website-crawler", "spider", "url-finder", "site-map-generator"]
categories = ["web", "accessibility", "crawler"]

[dependencies]
tokio = { version = "^1.18.2", features = [ "rt-multi-thread", "net", "macros", "time" ] }
tonic = "0.7.2"
prost = "0.10"
num_cpus = "1.13.0"
reqwest = { version = "0.11.10", features = ["blocking"] }
scraper = "0.13"
robotparser-fork = "0.10.5"
url = "2.2.2"
rayon = "1.5.2"
regex = { version = "^1.5.0", optional = true }
hashbrown = { version = "0.12" }
log = "0.4.16"
lazy_static = "1.4.0"

[[bin]]
name = "health_client"
path = "src/hc_client.rs"

[build-dependencies]
tonic-build = "0.7"

[dev-dependencies]
once_cell = "1.2.0"